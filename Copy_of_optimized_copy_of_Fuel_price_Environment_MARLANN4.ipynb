{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsarpongstreetor/rl/blob/main/Copy_of_optimized_copy_of_Fuel_price_Environment_MARLANN4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gorqaRJWGLD4",
        "outputId": "6fa1c6f2-8a64-4e79-ef33-5603d03c1161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchrl in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchrl) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchrl) (24.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.2.1)\n",
            "Requirement already satisfied: tensordict>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from torchrl) (0.5.0)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from tensordict>=0.5.0->torchrl) (3.10.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchrl) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchrl) (1.3.0)\n",
            "Requirement already satisfied: tensordict in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from tensordict) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensordict) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from tensordict) (2.2.1)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from tensordict) (3.10.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->tensordict) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->tensordict) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->tensordict) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->tensordict) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->tensordict) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->tensordict) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->tensordict) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.4.0->tensordict) (1.3.0)\n",
            "Requirement already satisfied: torchview in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.10/dist-packages (from pydrive) (2.137.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.10/dist-packages (from pydrive) (6.0.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (2.19.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (4.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.64.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.24.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.2->pydrive) (5.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.2->pydrive) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->pydrive) (2024.7.4)\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchrl\n",
        "!pip3 install tensordict\n",
        "!pip3 install torchview\n",
        "!pip3 install torchvision\n",
        "!pip3 install pydrive\n",
        "!pip3 install ipython-autotime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBdvI114pYjy",
        "outputId": "1201d721-a344-4b26-c46a-b7522d572bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import nn\n",
        "from tensordict.nn import TensorDictModule\n",
        "from tensordict.nn.distributions import NormalParamExtractor\n",
        "\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter,TransformedEnv )\n",
        "from torchrl.envs.libs.gym import GymEnv\n",
        "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
        "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
        "from torchrl.objectives import ClipPPOLoss\n",
        "from torchrl.objectives.value import GAE\n",
        "from tqdm import tqdm\n",
        "import google.colab\n",
        "import pygame\n",
        "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "from google.colab import drive\n",
        "google.colab.drive.mount('/content/drive')\n",
        "from collections import defaultdict\n",
        "from typing import Optional\n",
        "import torchrl\n",
        "import numpy as np\n",
        "from tensordict import TensorDict, TensorDictBase\n",
        "from torchrl.modules import MultiAgentConvNet\n",
        "from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec,DiscreteTensorSpec\n",
        "from torchrl.envs import (\n",
        "    CatTensors,\n",
        "    EnvBase,\n",
        "    Transform,\n",
        "    TransformedEnv,\n",
        "    UnsqueezeTransform,\n",
        ")\n",
        "\n",
        "from torchrl.envs.transforms.transforms import _apply_to_composite\n",
        "from torchrl.envs.utils import check_env_specs, step_mdp\n",
        "import tensordict as td\n",
        "from torchrl.envs import EnvBase\n",
        "from torch import  tensor\n",
        "from torchrl.envs.transforms import TransformedEnv\n",
        "import tensordict\n",
        "\n",
        "from torchrl.envs import RewardSum, TransformedEnv\n",
        "from torchrl.envs.libs.vmas import VmasEnv\n",
        "from torchrl.envs.utils import check_env_specs\n",
        "\n",
        "# Multi-agent network\n",
        "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
        "\n",
        "# Loss\n",
        "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
        "\n",
        "# Utils\n",
        "torch.manual_seed(0)\n",
        "from matplotlib import pyplot as plt\n",
        "from typing import Union, Sequence, Type\n",
        "\n",
        "import torch.nn as nn\n",
        "from torchrl.envs.utils import check_env_specs, step_mdp\n",
        "from torchrl.modules import ProbabilisticActor\n",
        "#####################################\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "collapsed": true,
        "id": "4JWCT6vHpY_d"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import numpy as np  # Import NumPy\n",
        "k=0\n",
        "\n",
        "class DDataenv():\n",
        "\n",
        "  #Initialize\n",
        "  def __init__(self):\n",
        "    # Load data here\n",
        "      self.DDDDataDic =np.empty((8,7), dtype=np.float32)\n",
        "      # We have 3 actions, corresponding to \"increase\", \"decrease\", \"no change \" in fuel price\n",
        "      #self.action_space = spaces.Discrete(3)\n",
        "      # Observations are dictionaries with the agent's Observation which are.\n",
        "      # Forex, Crude oil pric, Fuel price, reward, action\n",
        "      self.vvmm=np.empty((8,10), dtype=np.float32)\n",
        "      self.k=k\n",
        "\n",
        "  def _downl_Data(self):\n",
        "        with open('/content/drive/MyDrive/deep learning codes/EIAAPI_DOWNLOAD/solutions/mergedata/DataDic.pt','rb') as rpp:\n",
        "\n",
        "          DataDic = torch.load(rpp,weights_only=True)\n",
        "        DDataDic=DataDic[0]\n",
        "        DDDDataDic=DDataDic\n",
        "        return DDDDataDic\n",
        "\n",
        "  def _get_obs_stats(self):\n",
        "        self.vvmm_dict = {}\n",
        "        observation=[]\n",
        "        vvmm_dict={},\n",
        "        vvnn=[]\n",
        "        vvnv={}\n",
        "        DD=[]\n",
        "        DDDataDic=[]\n",
        "        aadd={}\n",
        "        aadd1={}\n",
        "        aadd2={}\n",
        "\n",
        "        aabb=[]\n",
        "        actionState=[]\n",
        "        rewardState=[]\n",
        "        obsState=[]\n",
        "        obsFuels=[]\n",
        "        rewardFuels=[]\n",
        "        actionFuels=[]\n",
        "        Envvstatesinput=[]\n",
        "        Envvfuelsinput=[]\n",
        "         #observation\n",
        "        DDDataDic=self._downl_Data()\n",
        "        observation=DDDataDic[np.random.choice(DDDataDic.shape[0], 1, replace=False),:].numpy().astype(np.float32)\n",
        "        observation.flatten()\n",
        "        aabb=pd.DataFrame(observation)\n",
        "        aabb.transpose()\n",
        "        aabb.columns=['Forex','WTI','Brent','OPEC','Fuelprice5','Fuelprice6','Fuelprice7','Fuelprice8','Fuelprice9','Fuelprice10','Fuelprice11','Fuelprice12','Fuelprice13',\n",
        "                     'reward0','reward1','reward2','reward3','reward4','reward5','reward6','reward7','reward8','reward9','reward10','reward11','reward12',\n",
        "                    'action0','action1','action2','action3','action4','action5','action6','action7','action8','action9','action10','action11','action12',]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        obsState=np.array(aabb.iloc[0,0:4])\n",
        "        rewardState=np.array(aabb.iloc[0,13:17])\n",
        "        actionState=np.array(aabb.iloc[0,26:30])\n",
        "        obsfuels=np.array(aabb.iloc[0,4:13])\n",
        "        rewardfuels=np.array(aabb.iloc[0,17:26])\n",
        "        actionfuels=np.array(aabb.iloc[0,30:39])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        aadd1=pd.concat((pd.Series(obsState),pd.Series(rewardState),pd.Series(actionState)),axis=1)\n",
        "\n",
        "        aadd1=pd.DataFrame(aadd1)\n",
        "        aadd1.columns=['obsState','rewardState','actionState']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        aadd2=pd.concat([pd.Series(obsfuels),pd.Series(rewardfuels),pd.Series(actionfuels)],axis=1)\n",
        "\n",
        "        aadd2=pd.DataFrame(aadd2)\n",
        "        aadd2.columns=['obsFuels','rewardFuels','actionFuels']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        obsState_max=[]\n",
        "        obsState_min=[]\n",
        "        rewardState_max=[]\n",
        "        rewardState_min=[]\n",
        "        actionState_max=[]\n",
        "        actionState_min=[]\n",
        "        obsfuels_max=[]\n",
        "        obsfuels_min=[]\n",
        "        rewardfuels_max=[]\n",
        "        rewardfuels_min=[]\n",
        "        actionfuels_max=[]\n",
        "        actionfuels_min=[]\n",
        "\n",
        "\n",
        "        DD = DDDataDic.clone()\n",
        "        DD.reshape(-1, DD.shape[-1])\n",
        "        vvmm=np.zeros((8,DD.shape[1]))\n",
        "        df = pd.DataFrame(DD)\n",
        "\n",
        "        vvmm = df.describe()\n",
        "        vvmm.columns=['Forex','WTI','Brent','OPEC','Fuelprice5','Fuelprice6','Fuelprice7','Fuelprice8','Fuelprice9','Fuelprice10','Fuelprice11','Fuelprice12','Fuelprice13',\n",
        "                      'reward0','reward1','reward2','reward3','reward4','reward5','reward6','reward7','reward8','reward9','reward10','reward11','reward12',\n",
        "                      'action0','action1','action2','action3','action4','action5','action6','action7','action8','action9','action10','action11','action12',]\n",
        "\n",
        "        vvmm.index=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
        "        vvmm1=pd.concat((vvmm.iloc[:,0:4],vvmm.iloc[:,13:17],vvmm.iloc[:,26:30]),axis=1)\n",
        "        vvmm2=pd.concat((vvmm.iloc[:,4:13] ,vvmm.iloc[:,17:26],vvmm.iloc[:,30:39]),axis=1)\n",
        "\n",
        "        vvmm1.index=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
        "        vvmm2.index=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ii= [3, 7]\n",
        "       # vvmm.astype(dtype=np.float32, copy=True, errors='list')\n",
        "        xx1=[]\n",
        "        yy1=[]\n",
        "        xxx1=[]\n",
        "        yyy1=[]\n",
        "        vvvv1=[]\n",
        "\n",
        "        for k in range(len(vvmm1.columns)):\n",
        "          for i in ii:\n",
        "            xx1.append(vvmm1.iat[i, k])\n",
        "\n",
        "\n",
        "        for k in range(len(vvmm1.columns)):\n",
        "          for i in ii:\n",
        "            yy1.append(str(vvmm1.index[i]) + '.' + str(vvmm1.columns[k]))\n",
        "\n",
        "        xx1=np.array(xx1)\n",
        "        xx1 = xx1.reshape(1,24)\n",
        "\n",
        "        yyy1=iter(yy1)\n",
        "        vvnn1=pd.DataFrame(xx1,dtype=np.float32)\n",
        "\n",
        "\n",
        "        vvnn1.columns=yyy1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for j in range(24):\n",
        "          if j < 4:\n",
        "            obsState_min.append(((vvnn1.iloc[0,2*j  :2*j+1]).squeeze()))\n",
        "            obsState_max.append(((vvnn1.iloc[0,2*j+1:2*j+2]).squeeze()))\n",
        "            rewardState_min.append(((vvnn1.iloc[0,2*j+8:2*j+9]).squeeze()))\n",
        "            rewardState_max.append(((vvnn1.iloc[0,2*j+9:2*j+10]).squeeze()))\n",
        "            actionState_min.append(((vvnn1.iloc[0,2*j+16:2*j+17]).squeeze()))\n",
        "            actionState_max.append(((vvnn1.iloc[0,2*j+17:2*j+18]).squeeze()))\n",
        "\n",
        "\n",
        "        ii= [3, 7]\n",
        "       # vvmm.astype(dtype=np.float32, copy=True, errors='list')\n",
        "        xx2=[]\n",
        "        yy2=[]\n",
        "        xxx2=[]\n",
        "        yyy2=[]\n",
        "        vvvv2=[]\n",
        "\n",
        "        for k in range(len(vvmm2.columns)):\n",
        "          for i in ii:\n",
        "            xx2.append(vvmm2.iat[i, k])\n",
        "\n",
        "\n",
        "        for k in range(len(vvmm2.columns)):\n",
        "          for i in ii:\n",
        "            yy2.append(str(vvmm2.index[i]) + '.' + str(vvmm2.columns[k]))\n",
        "\n",
        "        xx2=np.array(xx2)\n",
        "        xx2 = xx2.reshape(1,54)\n",
        "\n",
        "        yyy2=iter(yy2)\n",
        "        vvnn2=pd.DataFrame(xx2,dtype=np.float32)\n",
        "\n",
        "\n",
        "        vvnn2.columns=yyy2\n",
        "\n",
        "\n",
        "\n",
        "        for j in range(54):\n",
        "          if j < 9:\n",
        "            obsfuels_max.append(((vvnn2.iloc[0,2*j:2*j+1])))\n",
        "            obsfuels_min.append(((vvnn2.iloc[0,2*j+1:2*j+2])))\n",
        "\n",
        "            rewardfuels_min.append(((vvnn2.iloc[0,2*j+17:2*j+18])))\n",
        "            rewardfuels_max.append(((vvnn2.iloc[0,2*j+18:2*j+19])))\n",
        "\n",
        "            actionfuels_max.append(((vvnn2.iloc[0,2*j+36:2*j+37])))\n",
        "            actionfuels_min.append(((vvnn2.iloc[0,2*j+37:2*j+38])))\n",
        "\n",
        "        obsfuels_max=np.array(obsfuels_max).reshape(-1)\n",
        "        obsfuels_min=np.array(obsfuels_min).reshape(-1)\n",
        "        actionfuels_max=np.array(actionfuels_max).reshape(-1)\n",
        "        actionfuels_min=np.array(actionfuels_min).reshape(-1)\n",
        "        rewardfuels_max=np.array(rewardfuels_max).reshape(-1)\n",
        "        rewardfuels_min=np.array(rewardfuels_min).reshape(-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        vvmm1_dict=pd.concat([pd.Series(obsState_max),pd.Series(obsState_min),pd.Series(rewardState_max),pd.Series(rewardState_min),pd.Series(actionState_max),pd.Series(actionState_min)],axis=1)\n",
        "\n",
        "        vvmm1_dict.columns=['obsState_max','obsState_min','rewardState_max','rewardState_min','actionState_max','actionState_min']\n",
        "\n",
        "        vvmm2_dict=pd.concat([pd.Series(obsfuels_max),pd.Series(obsfuels_min),pd.Series(rewardfuels_max),pd.Series(rewardfuels_min),pd.Series(actionfuels_max),pd.Series(actionfuels_min)],axis=1)\n",
        "\n",
        "        vvmm2_dict.columns=['obsFuels_min','obsFuels_max','rewardFuels_min','rewardFuels_max','actionFuels_min','actionFuels_max']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        fv=pd.concat((vvmm2_dict,aadd2),axis=1)\n",
        "        sv=pd.concat((vvmm1_dict,aadd1),axis=1)\n",
        "        ssv=np.zeros((9,9))\n",
        "        ssv=pd.DataFrame(ssv)\n",
        "        ssv.columns=sv.columns\n",
        "        sssv=sv+ssv\n",
        "        sv=sssv\n",
        "        svfv=pd.concat((sv,fv),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #result=pd.concat([aadd1,vvmm1_dict], axis=1)\n",
        "        #result.columns=['obsState','actionState','reward','obsState_max','obsState_min','rewardState_max','rewardState_min','actionState_max','actionState_min']\n",
        "       # result2=pd.concat([aadd2,vvnn2], axis=1)\n",
        "       # resulttensor=torch.tensor(result.to_numpy())\n",
        "        #print(resulttensor)\n",
        "        result=svfv.to_dict(orient='list')\n",
        "\n",
        "        return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF--TTWOJGES"
      },
      "outputs": [],
      "source": [
        "my_object=DDataenv()\n",
        "rresult=my_object._get_obs_stats()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _step(tensordict):\n",
        "    # ...\n",
        "    td=env.gen_params()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Extract the necessary DataFrames from rresult\n",
        "    # Extract the variables needed in _make_spec\n",
        "    n_agents =  env.n_agents\n",
        "    agent_tds = []\n",
        "    agents = [{} for _ in range(n_agents)]\n",
        "    # Collect observations in a list first\n",
        "    # Iterate over the DataFrame\n",
        "\n",
        "    for i in range(n_agents):\n",
        "        # Initialize lists to store data for each agent within the batch\n",
        "        agent_i_obs = []\n",
        "        agent_i_action = []\n",
        "        agent_i_rew = []\n",
        "        agent_i_new_obs = []\n",
        "\n",
        "        agent_obs_list=[]\n",
        "        agent_reward_list=[]\n",
        "        agent_action_list=[]\n",
        "        newobs_obs_list=[]\n",
        "\n",
        "        for j in range(env.batch_size[0]):\n",
        "            obsState=td['params','obsState'].clone().detach()\n",
        "            rewardState=td['params','rewardState'].clone().detach()\n",
        "            actionState=td['params','actionState'].clone().detach()\n",
        "            obsFuels=td['params','obsFuels'].clone().detach().unsqueeze((-1))\n",
        "            rewardFuels=td['params','rewardFuels'].clone().detach().unsqueeze((-1))\n",
        "            actionFuels=td['params','actionFuels'].clone().detach().unsqueeze((-1))\n",
        "            obs=[]\n",
        "            reward=[]\n",
        "            action=[]\n",
        "            obsj=[]\n",
        "            rewardj=[]\n",
        "            actionj=[]\n",
        "            newobs=[]\n",
        "            obs=torch.cat(((torch.mul((obsState[0:4]),torch.ones(len(obsFuels),1))),obsFuels),dim=1)\n",
        "            reward=rewardFuels\n",
        "            action=actionFuels\n",
        "            for i,[agent_obs,agent_reward,agent_action], in enumerate(zip(obs,reward,action)):\n",
        "                agent_reward_list.append(agent_reward)\n",
        "                agent_action_list.append(agent_action)\n",
        "                newobs=torch.add(agent_obs,((agent_action)*agent_reward))\n",
        "                newobs_obs_list.append(newobs)\n",
        "\n",
        "                torch.stack(agent_reward_list, dim=1)\n",
        "                torch.stack(agent_action_list, dim=1)\n",
        "                torch.stack(newobs_obs_list, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            obsj.append(newobs_obs_list)\n",
        "            rewardj.append(agent_reward_list)\n",
        "            actionj.append(agent_action_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        agent_reward = torch.stack([torch.stack(inner_list) for inner_list in rewardj], dim=0)\n",
        "        agent_action = torch.stack([torch.stack(inner_list) for inner_list in actionj], dim=0)\n",
        "        agent_obs = torch.stack([torch.stack(inner_list) for inner_list in  obsj], dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Ensure agent_obs_tensor has the correct batch dimension\n",
        "        observation  = agent_obs.float()\n",
        "        episode_reward  = agent_reward.float()\n",
        "        agent_action_tensor =  agent_action.float()\n",
        "\n",
        "\n",
        "\n",
        "    dones = torch.zeros((env.batch_size[0], n_agents), dtype=torch.bool)\n",
        "    nextt = TensorDict({\n",
        "      \"agents\": {\n",
        "            \"observation\": observation[:, 0:n_agents, :], # Make sure 'observation' is included here\n",
        "            \"reward\":episode_reward[:, 0:n_agents, :],\n",
        "          },\n",
        "\n",
        "       \"terminated\": dones.clone(),\n",
        "\n",
        "\n",
        "    }, batch_size=env.batch_size, device=env.device)\n",
        "    return nextt\n",
        "\n",
        "\n",
        "\n",
        "def _reset(self, tensordict=None, **kwargs):\n",
        "\n",
        "    if tensordict is None:\n",
        "      td = self.gen_params(batch_size=self.batch_size)\n",
        "      obsState_max=td['params','obsState_max'].clone().detach()\n",
        "      obsState_min=td['params','obsState_min'].clone().detach()\n",
        "      obsFuels_max=td['params','obsFuels_max'].clone().detach()\n",
        "      obsFuels_min=td['params','obsFuels_min'].clone().detach()\n",
        "      n_agents = self.n_agents\n",
        "\n",
        "\n",
        "\n",
        "      # Initialize agent list here\n",
        "      batchtentensor=[]\n",
        "      agents = [{} for _ in range(self.n_agents)]\n",
        "      agents = [{\"id\": i, \"data\": torch.randn(5)} for i in range(self.n_agents)]\n",
        "      agent_obs_list = [] # Collect observations in a list first\n",
        "      agent_tds = []\n",
        "      agent_obs_tensor=[]\n",
        "\n",
        "\n",
        "      # Iterate over the DataFrame\n",
        "        # Iterate over the DataFrame\n",
        "       # Iterate over the DataFrame\n",
        "      random_numbers = torch.rand((self.batch_size[0],5), generator=self.rng, device=self.device)\n",
        "      low_x=torch.cat((torch.mul(obsState_min[:,0:4],torch.ones(9,1)),obsFuels_min.T ),dim=1)\n",
        "      high_x=torch.cat((torch.mul(obsState_max[:,0:4],torch.ones(9,1)),obsFuels_max.T ),dim=1)\n",
        "      obs =  torch.add(torch.mul(random_numbers, torch.add(high_x, -low_x)), low_x)\n",
        "      obs= obs.float()\n",
        "          # Iterate over the DataFrame\n",
        "\n",
        "      for i, agent_obs in enumerate(obs):\n",
        "          agent_obs_list.append(agent_obs)\n",
        "          # Ensure agent_obs_tensor has the correct batch dimension\n",
        "      agent_obs_tensor = torch.stack(agent_obs_list, dim=0).unsqueeze(0)\n",
        "        # Now agent_obs_tensor has shape (1, num_agents, 5)\n",
        "\n",
        "\n",
        "      dones = torch.zeros((self.batch_size[0], n_agents), dtype=torch.bool)\n",
        "\n",
        "      resett = TensorDict(\n",
        "        {\n",
        "        \"agents\": {  # Add \"agents\" key\n",
        "            \"observation\": agent_obs_tensor[:, 0:n_agents, :],  # Directly use agent_obs_tensor\n",
        "        },\n",
        "        \"terminated\": dones.clone(),\n",
        "\n",
        "    }, batch_size=self.batch_size[0], device=self.device)\n",
        "    return resett\n",
        "\n",
        "\n",
        "# @title Default title text\n",
        "def _make_spec(self, td_agents):\n",
        "    agent =[{}]*self.n_agents\n",
        "    action_specs = []\n",
        "    observation_specs = []\n",
        "    reward_specs = []\n",
        "\n",
        "    # Initialize result lists outside the loop\n",
        "\n",
        "\n",
        "    td = self.gen_params()\n",
        "    obsState_max=td_agents['params','obsState_max'].clone().detach()\n",
        "    obsState_min=td_agents['params','obsState_min'].clone().detach()\n",
        "    rewardState_max=td_agents['params','rewardState_max'].clone().detach()\n",
        "    rewardState_min=td_agents['params','rewardState_min'].clone().detach()\n",
        "    actionState_max=td_agents['params','actionState_max'].clone().detach()\n",
        "    actionState_min=td_agents['params','actionState_min'].clone().detach()\n",
        "    obsFuels_max=td_agents['params','obsFuels_max'].clone().detach()\n",
        "    obsFuels_min=td_agents['params','obsFuels_min'].clone().detach()\n",
        "    rewardFuels_max=td_agents['params','rewardFuels_max'].clone().detach()\n",
        "    rewardFuels_min=td_agents['params','rewardFuels_min'].clone().detach()\n",
        "    actionFuels_max=td_agents['params','actionFuels_max'].clone().detach()\n",
        "    actionFuels_min=td_agents['params','actionFuels_min'].clone().detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    low=torch.cat((torch.multiply(torch.ones((self.n_agents,1)),torch.tensor(self.rewstin.values)[:,None])),torch.reshape(torch.tensor(self.rewfmin.values),(self.n_agents, 1)),dim=1)\n",
        "    for i in range(self.n_agents):\n",
        "        agent[i][\"action_spec\"] = BoundedTensorSpec(low = actionFuels_min[i:i+1].reshape(1,),\n",
        "                                                     high = actionFuels_max[i:i+1].reshape(1,),\n",
        "                                                     shape=(1,),\n",
        "                                                     dtype=torch.float32),\n",
        "\n",
        "        agent[i][\"reward_spec\"] =  BoundedTensorSpec(low =rewardFuels_min[i:i+1].reshape(1,),\n",
        "                                                     high = rewardFuels_max[i:i+1].reshape(1,),\n",
        "                                                     shape=(1,),\n",
        "                                                     dtype=torch.float32),\n",
        "        agent[i][\"reward_spec\"] = BoundedTensorSpec(low = rewardFuels_min[i:i+1].reshape(1,),\n",
        "                                                     high = rewardFuels_max[i:i+1].reshape(1,),\n",
        "                                                     shape=(1,),\n",
        "                                                     dtype=torch.float32),\n",
        "\n",
        "\n",
        "\n",
        "        agent[i][\"observation_spec\"]  = BoundedTensorSpec(low = torch.cat((obsState_min[0:4],obsFuels_min[i:i+1]),1).reshape(1, 5),\n",
        "                                                          high = torch.cat((obsState_max[0:4],obsFuels_max[i:i+1]),1).reshape(1, 5),\n",
        "                                                          shape=(1,5),\n",
        "                                                          dtype=torch.float32),\n",
        "\n",
        "\n",
        "        action_specs.append(agent[i][\"action_spec\"])\n",
        "        reward_specs.append(agent[i][\"reward_spec\"])\n",
        "        observation_specs.append(agent[i][\"observation_spec\"])\n",
        "\n",
        "\n",
        "\n",
        "# Construct CompositeSpec objects with the correct nesting and batch size\n",
        "def _make_spec_updated(self, td_agents):\n",
        "    agent =[{}]*self.n_agents\n",
        "    action_specs = []\n",
        "    observation_specs = []\n",
        "    reward_specs = []\n",
        "\n",
        "    # Initialize result lists outside the loop\n",
        "    obsState_max=td_agents['params','obsState_max'].clone().detach()\n",
        "    obsState_min=td_agents['params','obsState_min'].clone().detach()\n",
        "    rewardState_max=td_agents['params','rewardState_max'].clone().detach()\n",
        "    rewardState_min=td_agents['params','rewardState_min'].clone().detach()\n",
        "    actionState_max=td_agents['params','actionState_max'].clone().detach()\n",
        "\n",
        "    actionState_min=td_agents['params','actionState_min'].clone().detach()\n",
        "    obsFuels_max=td_agents['params','obsFuels_max'].clone().detach()\n",
        "    obsFuels_min=td_agents['params','obsFuels_min'].clone().detach()\n",
        "    rewardFuels_max=td_agents['params','rewardFuels_max'].clone().detach()\n",
        "    rewardFuels_min=td_agents['params','rewardFuels_min'].clone().detach()\n",
        "    actionFuels_max=td_agents['params','actionFuels_max'].clone().detach()\n",
        "    actionFuels_min=td_agents['params','actionFuels_min'].clone().detach()\n",
        "    result55=[]\n",
        "    result55=[]\n",
        "    result44=[]\n",
        "    result33=[]\n",
        "    result22=[]\n",
        "    result11=[]\n",
        "    result00=[]\n",
        "    result555=[]\n",
        "    result444=[]\n",
        "    result333=[]\n",
        "    result222=[]\n",
        "    result111=[]\n",
        "    result000=[]\n",
        "    for i in range(self.n_agents):  # Make sure the loop iterates 9 times\n",
        "        result55.append(actionFuels_min[i:i+1])\n",
        "        result44.append(actionFuels_max[i:i+1])\n",
        "        result33.append(rewardFuels_min[i:i+1])\n",
        "        result22.append(rewardFuels_max[i:i+1])\n",
        "        result11.append(torch.cat((obsState_min[0:4],obsFuels_min[i:i+1]),0).reshape(1, 5))\n",
        "        result00.append(torch.cat((obsState_max[0:4],obsFuels_max[i:i+1]),0).reshape(1, 5))\n",
        "\n",
        "    result555= torch.stack(result55,dim=0) # Transpose dimensions 0 and 1\n",
        "    result444 =torch.stack(result44,dim=0)\n",
        "    result333 =torch.stack(result33,dim=0)\n",
        "    result222 =torch.stack(result22,dim=0)\n",
        "    result111 =torch.stack(result11,dim=0)\n",
        "    result000 =torch.stack(result00,dim=0)\n",
        "    print(result555)\n",
        "\n",
        "\n",
        "    self.unbatched_action_spec = CompositeSpec(\n",
        "    {\"agents\": CompositeSpec(\n",
        "            {\"action\": BoundedTensorSpec(\n",
        "                low=result555,  # Ensure these tensors have the correct shape\n",
        "                high=result444,\n",
        "                shape=(self.n_agents, 1),  # Explicitly set the shape here\n",
        "                dtype=torch.float32,\n",
        "             )}\n",
        "        )}\n",
        "    )\n",
        "\n",
        "\n",
        "    self.unbatched_reward_spec = CompositeSpec(\n",
        "    {\"agents\": CompositeSpec(\n",
        "            {\"reward\": BoundedTensorSpec(\n",
        "                low=result333,\n",
        "                high=result222,\n",
        "                shape=(self.n_agents,1),\n",
        "                dtype=torch.float32,\n",
        "             )}\n",
        "        )}\n",
        "    )\n",
        "\n",
        "    self.unbatched_observation_spec = CompositeSpec(  # Change to unbatched_observation_spec\n",
        "    {\"agents\": CompositeSpec(\n",
        "            {\"observation\": BoundedTensorSpec(\n",
        "                low=result111.squeeze(1),\n",
        "                high=result000.squeeze(1),\n",
        "                shape=(self.n_agents,5),\n",
        "                dtype=torch.float32,\n",
        "             )}\n",
        "        )}\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    self.unbatched_done_spec = DiscreteTensorSpec(n = 2,shape =torch.Size((self.n_agents,5)), dtype = torch.bool)\n",
        "\n",
        "    # Now you can expand the specs\n",
        "    self.unbatched_done_spec = DiscreteTensorSpec(\n",
        "        n=2, shape=torch.Size((self.n_agents,)), dtype=torch.bool\n",
        "    )\n",
        "\n",
        "    # Now you can expand the specs\n",
        "    self.action_spec = self.unbatched_action_spec.expand(\n",
        "        *self.batch_size, *self.unbatched_action_spec.shape\n",
        "    )\n",
        "    self.observation_spec = self.unbatched_observation_spec.expand(\n",
        "        *self.batch_size, *self.unbatched_observation_spec.shape\n",
        "    )  # Use unbatched_observation_spec\n",
        "    self.reward_spec = self.unbatched_reward_spec.expand(\n",
        "        *self.batch_size, *self.unbatched_reward_spec.shape\n",
        "    )\n",
        "    self.done_spec = self.unbatched_done_spec.expand(\n",
        "        *self.batch_size, *self.unbatched_done_spec.shape\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def make_composite_from_td(td):\n",
        "    # custom function to convert a ``tensordict`` in a similar spec structure\n",
        "    # of unbounded values.\n",
        "    composite = CompositeSpec(\n",
        "        {\n",
        "            key: make_composite_from_td(tensor)\n",
        "            if isinstance(tensor, TensorDictBase)\n",
        "            else UnboundedContinuousTensorSpec(\n",
        "                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n",
        "            )\n",
        "            for key, tensor in td.items()\n",
        "        },\n",
        "        shape=td.shape,\n",
        "    )\n",
        "    return composite\n",
        "\n",
        "\n",
        "\n",
        "def gen_params(batch_size=torch.Size()) -> TensorDictBase:\n",
        "    \"\"\"Returns a ``tensordict`` containing the input tensors.\"\"\"\n",
        "    if batch_size is None:\n",
        "      batch_size = []\n",
        "    my_object=DDataenv()\n",
        "    ac=my_object._get_obs_stats()\n",
        "    if batch_size:\n",
        "        # Assuming 'ac' is a dictionary of tensors, expand each tensor\n",
        "        ac = {k: torch.tensor(v).expand(*batch_size, *torch.tensor(v).shape)  for k, v in ac.items()} # Convert lists to tensors before expanding\n",
        "\n",
        "\n",
        "    td = TensorDict({\n",
        "\n",
        "          \"params\": ac,\n",
        "          }\n",
        "        ,\n",
        "        batch_size=batch_size,\n",
        "        device=torch.device(\"cpu\"),\n",
        "    )\n",
        "    if batch_size:\n",
        "      td = td.expand(batch_size).contiguous()\n",
        "    return td\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _set_seed(self, seed:45):\n",
        "    rng = torch.manual_seed(seed)\n",
        "    self.rng = rng\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AnFuelpriceEnv(EnvBase):\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": 30,\n",
        "    }\n",
        "    batch_locked = False\n",
        "\n",
        "    def __init__(self,td_params=None, seed=None, device=\"cpu\"):\n",
        "        if td_params is None:\n",
        "            td_params = self.gen_params()\n",
        "\n",
        "\n",
        "        # Extract the variables needed in _make_spec\n",
        "        self.n_agents = 2\n",
        "\n",
        "        self.agent_tds = []\n",
        "        self.agents = [{} for _ in range(self.n_agents)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        super().__init__(device=device, batch_size=[1])\n",
        "        self._make_spec(td_params)\n",
        "        if seed is None:\n",
        "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
        "        self.set_seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "    # Helpers: _make_step and gen_params\n",
        "    gen_params =staticmethod(gen_params)\n",
        "    _make_spec = _make_spec_updated\n",
        "    # Mandatory methods: _step, _reset and _set_seed\n",
        "    _reset = _reset\n",
        "    _step = staticmethod(_step)\n",
        "    _set_seed = _set_seed\n",
        "\n",
        "env = AnFuelpriceEnv()\n",
        "print(\"\\n*action_spec:\", env.full_action_spec)\n",
        "print(\"\\n*reward_spec:\", env.full_reward_spec)\n",
        "print(\"\\n*done_spec:\", env.full_done_spec)\n",
        "print(\"\\n*observation_spec:\", env.observation_spec)\n",
        "\n",
        "print(\"\\n-action_keys:\", env.action_keys)\n",
        "print(\"\\n-reward_keys:\", env.reward_keys)\n",
        "print(\"\\n-done_keys:\", env.done_keys)\n",
        "\n",
        "print(\"input_spec:\", env.input_spec)\n",
        "print(\"action_spec (as defined by input_spec):\", env.action_spec)\n",
        "print(\"reward_spec:\", env.reward_spec)\n",
        "td = env.reset()\n",
        "print(\"reset tensordict\", td)\n",
        "check_env_specs(env)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBsO9V22bnuW",
        "outputId": "cc219416-7858-4b46-89a2-4fa325458cca"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [0.]], dtype=torch.float64)\n",
            "\n",
            "*action_spec: CompositeSpec(\n",
            "    agents: CompositeSpec(\n",
            "        action: BoundedTensorSpec(\n",
            "            shape=torch.Size([1, 2, 1]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
            "            device=cpu,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cpu,\n",
            "        shape=torch.Size([1])),\n",
            "    device=cpu,\n",
            "    shape=torch.Size([1]))\n",
            "\n",
            "*reward_spec: CompositeSpec(\n",
            "    agents: CompositeSpec(\n",
            "        reward: BoundedTensorSpec(\n",
            "            shape=torch.Size([1, 2, 1]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
            "            device=cpu,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cpu,\n",
            "        shape=torch.Size([1])),\n",
            "    device=cpu,\n",
            "    shape=torch.Size([1]))\n",
            "\n",
            "*done_spec: CompositeSpec(\n",
            "    done: DiscreteTensorSpec(\n",
            "        shape=torch.Size([1, 2]),\n",
            "        space=DiscreteBox(n=2),\n",
            "        device=cpu,\n",
            "        dtype=torch.bool,\n",
            "        domain=discrete),\n",
            "    terminated: DiscreteTensorSpec(\n",
            "        shape=torch.Size([1, 2]),\n",
            "        space=DiscreteBox(n=2),\n",
            "        device=cpu,\n",
            "        dtype=torch.bool,\n",
            "        domain=discrete),\n",
            "    device=cpu,\n",
            "    shape=torch.Size([1]))\n",
            "\n",
            "*observation_spec: CompositeSpec(\n",
            "    agents: CompositeSpec(\n",
            "        observation: BoundedTensorSpec(\n",
            "            shape=torch.Size([1, 2, 5]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([1, 2, 5]), device=cpu, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([1, 2, 5]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
            "            device=cpu,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cpu,\n",
            "        shape=torch.Size([1])),\n",
            "    device=cpu,\n",
            "    shape=torch.Size([1]))\n",
            "\n",
            "-action_keys: [('agents', 'action')]\n",
            "\n",
            "-reward_keys: [('agents', 'reward')]\n",
            "\n",
            "-done_keys: ['done', 'terminated']\n",
            "input_spec: CompositeSpec(\n",
            "    full_state_spec: CompositeSpec(\n",
            "    ,\n",
            "        device=cpu,\n",
            "        shape=torch.Size([1])),\n",
            "    full_action_spec: CompositeSpec(\n",
            "        agents: CompositeSpec(\n",
            "            action: BoundedTensorSpec(\n",
            "                shape=torch.Size([1, 2, 1]),\n",
            "                space=ContinuousBox(\n",
            "                    low=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
            "                    high=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
            "                device=cpu,\n",
            "                dtype=torch.float32,\n",
            "                domain=continuous),\n",
            "            device=cpu,\n",
            "            shape=torch.Size([1])),\n",
            "        device=cpu,\n",
            "        shape=torch.Size([1])),\n",
            "    device=cpu,\n",
            "    shape=torch.Size([1]))\n",
            "action_spec (as defined by input_spec): BoundedTensorSpec(\n",
            "    shape=torch.Size([1, 2, 1]),\n",
            "    space=ContinuousBox(\n",
            "        low=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
            "        high=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
            "    device=cpu,\n",
            "    dtype=torch.float32,\n",
            "    domain=continuous)\n",
            "reward_spec: BoundedTensorSpec(\n",
            "    shape=torch.Size([1, 2, 1]),\n",
            "    space=ContinuousBox(\n",
            "        low=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
            "        high=Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
            "    device=cpu,\n",
            "    dtype=torch.float32,\n",
            "    domain=continuous)\n",
            "reset tensordict TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                observation: Tensor(shape=torch.Size([1, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
            "            batch_size=torch.Size([1]),\n",
            "            device=cpu,\n",
            "            is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([1]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-29 23:34:16,213 [torchrl][INFO] check_env_specs succeeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j20g_A2DL5xi"
      },
      "source": [
        "Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6lB771bSJiU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnh1ly08MKSh",
        "outputId": "005990a3-3e47-41ea-f0c5-799165868e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalization constant shape: torch.Size([1, 2, 5])\n",
            "rollout of three steps: TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                action: Tensor(shape=torch.Size([1, 30, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                episode_reward: Tensor(shape=torch.Size([1, 30, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                observation: Tensor(shape=torch.Size([1, 30, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
            "            batch_size=torch.Size([1, 30]),\n",
            "            device=cpu,\n",
            "            is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([1, 30, 2]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                agents: TensorDict(\n",
            "                    fields={\n",
            "                        episode_reward: Tensor(shape=torch.Size([1, 30, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                        observation: Tensor(shape=torch.Size([1, 30, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                        reward: Tensor(shape=torch.Size([1, 30, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
            "                    batch_size=torch.Size([1, 30]),\n",
            "                    device=cpu,\n",
            "                    is_shared=False),\n",
            "                done: Tensor(shape=torch.Size([1, 30, 2]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                step_count: Tensor(shape=torch.Size([1, 30, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
            "                terminated: Tensor(shape=torch.Size([1, 30, 2]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "            batch_size=torch.Size([1, 30]),\n",
            "            device=cpu,\n",
            "            is_shared=False),\n",
            "        step_count: Tensor(shape=torch.Size([1, 30, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([1, 30, 2]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([1, 30]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "Shape of the rollout TensorDict: torch.Size([1, 30])\n"
          ]
        }
      ],
      "source": [
        "env = TransformedEnv(\n",
        "    env,\n",
        "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
        "    )\n",
        "\n",
        "\n",
        "#base_env1 = env\n",
        "#env = TransformedEnv(\n",
        "   # base_env1,\n",
        "    # ``Unsqueeze`` the observations that we will concatenate\n",
        "    #UnsqueezeTransform(\n",
        "        #unsqueeze_dim=-2,\n",
        "        # Access 'done' under the 'agents' key\n",
        "       # in_keys=[(\"agents\", \"observation\"),],\n",
        "       # in_keys_inv=[ (\"agents\", \"observation\")],\n",
        "   # ),\n",
        "#)\n",
        "\n",
        "\n",
        "base_env = env\n",
        "\n",
        "env = TransformedEnv(\n",
        "   base_env,\n",
        "    Compose(\n",
        "        ObservationNorm(in_keys=[(\"agents\", \"observation\")],),\n",
        "        DoubleToFloat(),\n",
        "        StepCounter(),\n",
        "    )\n",
        ")\n",
        "\n",
        "# Access the ObservationNorm transform (index 1)\n",
        "env.transform[1].init_stats(num_iter=60, reduce_dim=1, cat_dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " #Access the base environment through the 'env' attribute\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"normalization constant shape:\", env.transform[1].loc.shape)\n",
        "torch.numel(env.transform[1].loc)\n",
        "normashape = env.transform[1].loc.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Devices\n",
        "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Sampling\n",
        "frames_per_batch = 625 # Number of team frames collected per training iteration\n",
        "n_iters = 10  # Number of sampling and training iterations\n",
        "total_frames = frames_per_batch * n_iters\n",
        "\n",
        "# Training\n",
        "num_epochs = 20  # Number of optimization steps per training iteration\n",
        "minibatch_size = 60  # Size of the mini-batches in each optimization step\n",
        "lr = 3e-4  # Learning rate\n",
        "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
        "\n",
        "# PPO\n",
        "clip_epsilon = 0.2  # clip value for PPO loss\n",
        "gamma = 0.99  # discount factor\n",
        "lmbda = 0.9  # lambda for generalised advantage estimation\n",
        "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss\n",
        "\n",
        "\n",
        "n_rollout_steps = 30\n",
        "rollout = env.rollout(n_rollout_steps)\n",
        "print(\"rollout of three steps:\", rollout)\n",
        "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)\n",
        "##ttps://pytorch.org/rl/main/_modules/torchrl/modules/models/multiagent.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOoFn2szMPiL",
        "outputId": "a151a979-b09f-40d1-9882-bc9525f6e29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                episode_reward: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                observation: Tensor(shape=torch.Size([1, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
            "            batch_size=torch.Size([1]),\n",
            "            device=cpu,\n",
            "            is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        step_count: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([1]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "<class 'tensordict._td.TensorDict'>\n"
          ]
        }
      ],
      "source": [
        "reset_output = env.reset()\n",
        "print(reset_output)\n",
        "print(type(reset_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl77RTVXRnZf"
      },
      "source": [
        "## Policy Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_spec[\"agents\", \"observation\"].shape,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHqR-_6huEtw",
        "outputId": "18441176-fe6b-4459-ae8c-2ff043f326e1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 2, 5]),)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "5obzK17y4VCU"
      },
      "outputs": [],
      "source": [
        "\n",
        "share_parameters_policy = False\n",
        "\n",
        "policy_net = torch.nn.Sequential(\n",
        "    MultiAgentMLP(\n",
        "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],  # n_obs_per_agent\n",
        "        n_agent_outputs=2 * env.action_spec.shape[-1],  # 2 * n_actions_per_agents\n",
        "        n_agents=env.n_agents,\n",
        "        centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
        "        share_params=share_parameters_policy,\n",
        "        device=device,\n",
        "        depth=2,\n",
        "        num_cells=56,\n",
        "        activation_class=torch.nn.Tanh,\n",
        "    ),\n",
        "    NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "SbE_BSxo1BZ6"
      },
      "outputs": [],
      "source": [
        "policy_net.to(device)\n",
        "policy_module = TensorDictModule(\n",
        "    policy_net,\n",
        "    in_keys=[(\"agents\", \"observation\")],\n",
        "    out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "YVDHHiMn1H-u"
      },
      "outputs": [],
      "source": [
        "policy = ProbabilisticActor(\n",
        "    module=policy_module,\n",
        "    spec=env.unbatched_action_spec,\n",
        "    in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
        "    out_keys=[env.action_key],\n",
        "    distribution_class=TanhNormal,\n",
        "    distribution_kwargs={\n",
        "        \"low\": env.unbatched_action_spec[env.action_key].space.low,\n",
        "        \"high\": env.unbatched_action_spec[env.action_key].space.high,\n",
        "    },\n",
        "    return_log_prob=True,\n",
        "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
        ")  # we'll need the log-prob for the PPO loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imSb5_WISdUJ"
      },
      "source": [
        "Critc Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "395qPJVLv4by"
      },
      "outputs": [],
      "source": [
        "share_parameters_critic = False\n",
        "mappo = False  # IPPO if False\n",
        "\n",
        "critic_net = MultiAgentMLP(\n",
        "    n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
        "    n_agent_outputs=1,  # 1 value per agent\n",
        "    n_agents=env.n_agents,\n",
        "    centralised=mappo,\n",
        "    share_params=share_parameters_critic,\n",
        "    device=device,\n",
        "    depth=2,\n",
        "    num_cells=56,\n",
        "    activation_class=torch.nn.Tanh,\n",
        ")\n",
        "\n",
        "critic = TensorDictModule(\n",
        "    module=critic_net,\n",
        "    in_keys=[(\"agents\", \"observation\")],\n",
        "    out_keys=[(\"agents\", \"state_value\")],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNjZ6_1c1qD9",
        "outputId": "47c9b5e5-6640-4e05-fd5a-6bf9e9a47fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running policy: TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                action: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                episode_reward: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                loc: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                observation: Tensor(shape=torch.Size([1, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                sample_log_prob: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                scale: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
            "            batch_size=torch.Size([1]),\n",
            "            device=cpu,\n",
            "            is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        step_count: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([1]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "Running value: TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                episode_reward: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                observation: Tensor(shape=torch.Size([1, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                state_value: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
            "            batch_size=torch.Size([1]),\n",
            "            device=cpu,\n",
            "            is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        step_count: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([1]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "print(\"Running policy:\", policy(env.reset()))\n",
        "print(\"Running value:\", critic(env.reset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5D7oQxIXUIR"
      },
      "source": [
        "Data collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "I3uFCQXy3ZHT"
      },
      "outputs": [],
      "source": [
        "collector = SyncDataCollector(\n",
        "    env,\n",
        "    policy,\n",
        "    device=device,\n",
        "    storing_device=device,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Metjx9zXrgw"
      },
      "source": [
        "Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "KkVwE-COMZ92"
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer(\n",
        "    storage=LazyTensorStorage(\n",
        "        frames_per_batch, device=device\n",
        "    ),  # We store the frames_per_batch collected at each iteration\n",
        "    sampler=SamplerWithoutReplacement(),\n",
        "    batch_size=minibatch_size,  # We will sample minibatches of this size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qb6cpzYBUk"
      },
      "source": [
        "Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "5hc5PDFi_Ihf"
      },
      "outputs": [],
      "source": [
        "loss_module = ClipPPOLoss(\n",
        "    actor_network=policy,\n",
        "    critic_network=critic,\n",
        "    clip_epsilon=clip_epsilon,\n",
        "    entropy_coef=entropy_eps,\n",
        "    normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
        ")\n",
        "loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
        "    reward=env.reward_key,\n",
        "    action=env.action_key,\n",
        "    sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
        "    value=(\"agents\", \"state_value\"),\n",
        "    # These last 2 keys will be expanded to match the reward shape\n",
        "    done=(\"agents\", \"done\"),\n",
        "    terminated=(\"agents\", \"terminated\"),\n",
        ")\n",
        "\n",
        "\n",
        "loss_module.make_value_estimator(\n",
        "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
        ")  # We build GAE\n",
        "GAE = loss_module.value_estimator\n",
        "\n",
        "optim = torch.optim.Adam(loss_module.parameters(), lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o2hcK2g_J8re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeuDXQSPOF1i",
        "outputId": "49e3be85-d71e-4d79-f51a-d2383b976f90"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo3djtViYhQ4"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logs = defaultdict(list)\n",
        "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
        "eval_str = \"\"\n",
        "episode_reward_mean_list = []\n",
        "for tensordict_data in collector:\n",
        "    tensordict_data.set(\n",
        "        (\"next\", \"agents\", \"done\"),\n",
        "        tensordict_data.get((\"next\", \"done\"))\n",
        "        .unsqueeze(-1)\n",
        "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
        "    )\n",
        "    tensordict_data.set(\n",
        "        (\"next\", \"agents\", \"terminated\"),\n",
        "        tensordict_data.get((\"next\", \"terminated\"))\n",
        "        .unsqueeze(-1)\n",
        "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
        "    )\n",
        "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        GAE(\n",
        "            tensordict_data,\n",
        "            params=loss_module.critic_network_params,\n",
        "            target_params=loss_module.target_critic_network_params,\n",
        "        )  # Compute GAE and add it to the data\n",
        "\n",
        "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
        "    replay_buffer.extend(data_view)\n",
        "\n",
        "    for _ in range(num_epochs):\n",
        "        for _ in range(frames_per_batch // minibatch_size):\n",
        "            subdata = replay_buffer.sample()\n",
        "            loss_vals = loss_module(subdata)\n",
        "\n",
        "            loss_value = (\n",
        "                loss_vals[\"loss_objective\"]\n",
        "                + loss_vals[\"loss_critic\"]\n",
        "                + loss_vals[\"loss_entropy\"]\n",
        "            )\n",
        "\n",
        "            loss_value.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                loss_module.parameters(), max_grad_norm\n",
        "            )  # Optional\n",
        "\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "    collector.update_policy_weights_()\n",
        "\n",
        "    # Logging\n",
        "    done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
        "    episode_reward_mean = (\n",
        "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
        "    )\n",
        "    episode_reward_mean_list.append(episode_reward_mean)\n",
        "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
        "    pbar.update()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buwExczCVUnA",
        "outputId": "8ed834c1-8b7d-4a7d-c170-b452871d7e59"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "episode_reward_mean = 0:   0%|          | 0/10 [13:14<?, ?it/s]\n",
            "episode_reward_mean = nan: 100%|██████████| 10/10 [13:32<00:00, 76.12s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziRrLoXhYYo7"
      },
      "source": [
        "Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "SRc3b9Oe9nOs",
        "outputId": "4e7e3651-fdd9-4604-9a99-edf19898fd56"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(episode_reward_mean_list)\n",
        "plt.xlabel(\"Training iterations\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Episode reward mean\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZyaUl93Zxiz"
      },
      "source": [
        "Save Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xoHNdx9WZe87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ff922d-09ad-44ad-d87d-75c2100d0393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "PATH1 = '/content/drive/MyDrive/deep learning codes/EIAAPI_DOWNLOAD/solutions/mergedata/State_dict_model1.pt'\n",
        "PATH = '/content/drive/MyDrive/deep learning codes/EIAAPI_DOWNLOAD/solutions/mergedata/model.pt'\n",
        "torch.save({'actor_net_state_dict': policy_module.state_dict(),\n",
        "    'value_net_state_dict': critic_net.state_dict(),\n",
        "\n",
        "}, PATH1)\n",
        "\n",
        "torch.save({policy_module, critic_net}, PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYIoeMCuaEIF"
      },
      "source": [
        "Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Cw7-GBN-Z4Il",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "ac47d2b6-49c6-4aa7-ba0f-81e34c900d02"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options \n\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL builtins.set was not an allowed global by default. Please use `torch.serialization.add_safe_globals([set])` to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-cd42687f90c2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/deep learning codes/EIAAPI_DOWNLOAD/solutions/mergedata/model.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcritic_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                                      **pickle_load_args)\n\u001b[1;32m   1095\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m                 return _load(\n\u001b[1;32m   1098\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options \n\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL builtins.set was not an allowed global by default. Please use `torch.serialization.add_safe_globals([set])` to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "\n",
        "# Load\n",
        "PATH = '/content/drive/MyDrive/deep learning codes/EIAAPI_DOWNLOAD/solutions/mergedata/model.pt'\n",
        "critic_net, policy_module = torch.load(PATH,weights_only=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOLuPecjm6_5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGanWukPm9tV"
      },
      "source": [
        "PyTorch Model Updating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxExVuhvm6l6"
      },
      "outputs": [],
      "source": [
        "https://clear.ml/docs/latest/docs/guides/frameworks/pytorch/model_updating/#:~:text=To%20update%20a%20model%2C%20use,task%20as%20the%20output%20model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHLvcRQGnJt7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZN0kyVOnKkl"
      },
      "outputs": [],
      "source": [
        "k=3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DDDDataDic =np.empty((8,7), dtype=np.float32)\n",
        "# We have 3 actions, corresponding to \"increase\", \"decrease\", \"no change \" in fuel price\n",
        "#self.action_space = spaces.Discrete(3)\n",
        "# Observations are dictionaries with the agent's Observation which are.\n",
        "# Forex, Crude oil pric, Fuel price, reward, action\n",
        "\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/deep learning codes/EIAAPI_DOWNLOAD/solutions/mergedata/DataDic.pt','rb') as rpp:\n",
        "  DataDic = torch.load(rpp)\n",
        "DDataDic=DataDic[0]\n",
        "DDDDataDic=DDataDic[:,:,k]\n",
        "print(DDDDataDic)\n",
        "\n",
        "DDDDataDic_array = DDDDataDic.numpy()\n",
        "DDDDataDic_df = pd.DataFrame(DDDDataDic_array)\n",
        "DDDDataDic_df.to_csv('/content/drive/MyDrive/deep learning codes/EIAAPI_DOWNLOAD/solutions/mergedata/MatlabARIMAXAData.csv')\n",
        "\n",
        "n = len(DDDDataDic_df)\n",
        "# Create a DatetimeIndex with 7795 dates\n",
        "iddx=pd.date_range(start='2003-01-02', periods=n, freq='D')\n",
        "# Assign the DatetimeIndex to the 'date' column of DDDDataDic_df\n",
        "# Set the 'date' column as the index of DDDDataDic_df\n",
        "DDDDataDic_df.index = iddx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########\n",
        "DDDDataDic_df\n",
        "DDDDataDic_df.columns = ['US$_SDR', 'Brent', 'WTI','OPEC','Fuelprice','Rewards','Actions']\n",
        "#Selected attribute and target variables\n",
        "x =DDDDataDic_df[['US$_SDR', 'Brent', 'WTI','OPEC']]\n",
        "y = DDDDataDic_df['Fuelprice']"
      ],
      "metadata": {
        "id": "fM0wORQGnvUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of future time steps to forecast\n",
        "num_forecast_steps = 30\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming DDDDataDic_df is a pandas DataFrame with a DatetimeIndex\n",
        "last_date_30_Daysb = DDDDataDic_df.index[-1] - pd.Timedelta(days=num_forecast_steps)\n",
        "historical_data=DDDDataDic_df\n",
        "historical_data = DDDDataDic_df.loc[last_date_30_Daysb:]\n",
        "# we use the observation data historical_data =[\"US$_SDR\", \"Brent\", \"WTI\",\"OPEC\", \"Fuelprice\"]for the forecasting\n",
        "historical_data_obs = historical_data.columns[0:5]\n",
        "#print(historical_data_obs)\n",
        "\n",
        "# Convert to NumPy and remove singleton dimensions\n",
        "#sequence_to_plot = X_test.squeeze().cpu().numpy()\n",
        "\n",
        "# Use the last 30 data points as the starting point\n",
        "#historical_data_obs= sequence_to_plot[-1]\n",
        "print(historical_data_obs.shape)\n",
        "\n",
        "# Initialize a list to store the forecasted values\n",
        "forecasted_values = []\n",
        "forecasted_actions = []\n",
        "\n",
        "# Use the trained model to forecast future values\n",
        "with torch.no_grad():\n",
        "\tfor _ in range(num_forecast_steps*2):\n",
        "\t\t# Prepare the historical_data tensor\n",
        "\t\thistorical_data_tensor = torch.as_tensor(historical_data_obs).view(1,5).float().to(device)\n",
        "\t\t# Use the model to predict the next value\n",
        "\t\tpredicted_value = value_module(historical_data_tensor)\n",
        "\t\tpredicted_actions =policy_module(historical_data_tensor)\n",
        "\n",
        "\t\t# Append the predicted value to the forecasted_values list\n",
        "\t\tforecasted_values.append(predicted_value[0])\n",
        "\t\tforecasted_actions.append(predicted_actions[0])\n",
        "\t\tforecasted_reward=predicted_value[Fuelprice]-historical_data_obs[Fuelprice]\n",
        "\n",
        "\t\tpredicted_valueful=concat([predicted_value,forecasted_reward,predicted_actions])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\t# Update the historical_data sequence by removing the oldest value and adding the predicted value\n",
        "\t\thistorical_data = np.roll(historical_data, shift=-1)\n",
        "\t\thistorical_data[-1] = predicted_valueful\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate futute date\n",
        "last_datea =historical_data.index[-1]\n",
        "\n",
        "# Generate the next 30 dates\n",
        "future_dates = pd.date_range(start=last_date + pd.DateOffset(1), periods=30)\n",
        "\n",
        "# Concatenate the original index with the future dates\n",
        "combined_index = historical_data.index.append(future_dates)\n"
      ],
      "metadata": {
        "id": "c0Vmgcxqn3KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "57L7l5I5oN5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "PATH = '/content/drive/MyDrive/deep learning codes/EIAAPI_DOWNLOAD/solutions/mergedata/model.pt'\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "\n",
        "new_actor_net_dict = model.state_dict()\n",
        "new_value_net_dict = model.state_dict()\n",
        "\n",
        "\n",
        "pretrained_weights_new_actor_net= {}\n",
        "pretrained_weights_new_value_net= {}\n",
        "\n",
        "\n",
        "for key, value in checkpoint['actor_net_state_dict'].items():\n",
        "    new_key = key.replace('module.', '')\n",
        "    pretrained_weights_new_actor_net[new_key] = value\n",
        "\n",
        "#new_value_net_state_dict = {}\n",
        "for key, value in checkpoint['value_net_state_dict'].items():\n",
        "    new_key1 = key.replace('module.', '')\n",
        "    pretrained_weights_new_value_net[new_key1] = value\n",
        "\n",
        "new_actor_net_dict.update(pretrained_weights_new_actor_net)\n",
        "new_value_net_dict.update(pretrained_weights_new_value_net)\n",
        "\n",
        "model.load_state_dict(pretrained_weights_new_actor_net, strict = False)\n",
        "model.load_state_dict(pretrained_weights_new_value_net, strict = False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N0W78NdToOSF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6bf83e69-a724-4baa-9e00-b472d28c6913"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-998e4d4aabb7>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(PATH)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-998e4d4aabb7>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mnew_actor_net_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mnew_value_net_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_path = torch.load(path to pretrained model)\n",
        "\n",
        "\n",
        "new_model_dict = model.state_dict()\n",
        "\n",
        "pretrained_weights = { k:v for k , v in pretrained_path.items() if k in new_model_dict}\n",
        "\n",
        "new_model_dict.update(pretrained_weights)\n",
        "\n",
        "model.load_state_dict(pretrained_weights, strict = False)"
      ],
      "metadata": {
        "id": "YHMreTLvoTjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_dict.update(pretrained_weights)"
      ],
      "metadata": {
        "id": "BR3j0-feoZMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_value_net.load_state_dict(new_value_net_state_dict)\n",
        "new_actor_net.load_state_dict(new_actor_net_state_dict)"
      ],
      "metadata": {
        "id": "1Aoli_DXokSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_value_net.update(new_value_net_state_dict)\n",
        "new_actor_net.update(new_actor_net_state_dict)"
      ],
      "metadata": {
        "id": "oTB6JPtAopTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_actor_net)"
      ],
      "metadata": {
        "id": "yAqL9LfDosBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forecasting With models"
      ],
      "metadata": {
        "id": "3FlFQZmTpObG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w25xP214pCED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz\n",
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "DmNPY0qopFkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1822feb-188b-44e5-c7c7-627c45b61fba"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.4.0+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchview"
      ],
      "metadata": {
        "id": "36buwjsdpUwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a40f6b7-cc92-46d4-aae7-bd33423f3705"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchview in /usr/local/lib/python3.10/dist-packages (0.2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchview import draw_graph\n",
        "batch_size = frames_per_batch\n",
        "\n",
        "model_graph = draw_graph(critic_net, input_size=(1, 6250, 2, 5), device=device)\n",
        "model_graph.visual_graph\n",
        "# device='meta' -> no memory is consumed for visualization\n",
        "#model_graph1 = draw_graph(value_net, input_size=(780,1,5), device=device)\n",
        "#model_graph1.visual_graph\n"
      ],
      "metadata": {
        "id": "0POnsdXLpeJD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "outputId": "b7dc305c-efbd-4c38-8520-b1480d4085ab"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to run torchgraph see error message",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/torchview.py\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(model, x, device, model_graph, mode, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/recorder_tensor.py\u001b[0m in \u001b[0;36m_module_forward_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0minput_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         cur_node = ModuleNode(\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_nodes\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/computation_node/compute_node.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, module_unit, depth, parents, children, name, output_nodes)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_generator_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_unit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_container\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_unit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensordict/base.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Converting a tensordict to boolean value is not permitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Converting a tensordict to boolean value is not permitted",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-caec848245af>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes_per_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# device='meta' -> no memory is consumed for visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/torchview.py\u001b[0m in \u001b[0;36mdraw_graph\u001b[0;34m(model, input_data, input_size, graph_name, depth, device, dtypes, mode, strict, expand_nested, graph_dir, hide_module_functions, hide_inner_tensors, roll, show_shapes, save_graph, filename, directory, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     forward_prop(\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_recorder_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_record_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/torchview.py\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(model, x, device, model_graph, mode, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown input type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;34m\"Failed to run torchgraph see error message\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         ) from e\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchgraph see error message"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "graphviz.set_jupyter_format('png')\n",
        "#graphviz.Source(model_graph1.visual_graph)\n",
        "graphviz.Source(model_graph.visual_graph)"
      ],
      "metadata": {
        "id": "SttuzUgrpkbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_graph = draw_graph(actor_net, input_size=(7800,1,5), device=device)"
      ],
      "metadata": {
        "id": "miHsnWsNpzwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_graph.visual_graph)"
      ],
      "metadata": {
        "id": "OGUTeIFup34K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9u1EmeDygV4UUWhK1iatP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}